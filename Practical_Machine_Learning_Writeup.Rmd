
Practical Machine Learning - Course Project
==========================================================

Aron Y Joon (August 14, 2016)
==============================

***
# Introduction
Use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, this project aims to build a machine learning algorithm to predic the "classe" variable in the data set. This is part of the "Practical Machine Learning" course at Coursera.

***
# Background
(An excerpt from the course project information) Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har), under the section on the **Weight Lifting Exercise Dataset**.

The training data are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)


***
# Methods
This report mainly utelizes the R (version 3.2.0) with library **caret** (caret_6.0-70) together with multi-core parallel computing to help the efficiency. The version of R 
*R version 3.2.0 (2015-04-16)

```{r echo=TRUE, message=TRUE}
library(caret); library(corrplot)

```

***
# Analysis

## I. Data Cleaning
The first initial exploration of the data set showed that many variables should be excluded, such as the first variable (row number) and many with NAs.

```{r echo=TRUE, message=TRUE, fig.width=4, fig.height=4}
##load in the data from the URL

training <- read.csv(url('http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'), na.strings=c("NA","#DIV/0!",""))
save(training, file = "training.RData")  #save on hard disk for future reference
dim(training)

test <- read.csv(url('http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'), na.strings=c("NA","#DIV/0!",""))
save(test, file = "test.RData")		#save on hard disk for future reference
dim(test)

### remove the 1st variable since it's the row number
training=training[,-1]

### Check to see any variables with NAs
percent.NA.per.variable =  colSums(is.na(training)) / nrow(training) 
hist(percent.NA.per.variable, main="", xlab="% NA per variable")		# --> Obvious bimodality, with 60 variables with almost no missing (< 3%), and 100 completely missing
```

Hence, the variables with too many NAs were removed.

```{r echo=TRUE, message=TRUE}
###remove variables with too many NAs
variables.with.too.many.NAs =  colSums(is.na(training)) / nrow(training) > 0.9
training=training[,variables.with.too.many.NAs==F]
```
Furthermore, the variables with NZV (Nearly Zero Variance) were removed.

```{r echo=TRUE, message=TRUE}
# remove variables with Nearly Zero Variance
variables.with.NZV <- nearZeroVar(training)
training <- training[, -variables.with.NZV]

dim(training) #--> 58 variables, with classe the last variable to be predicted
```

Just in case, let us check the colinrarity between potential numeric predictors.

```{r echo=TRUE, message=TRUE, fig.width=6, fig.height=6}
##check correlation among the numeric predictors, excluding the first few identity and time stamp related variables
Matrix.of.correlation <- cor(training[, -c(1:4,58)])
corrplot(Matrix.of.correlation, order = "FPC", method = "color", type = "lower", tl.cex = 0.7, tl.col = rgb(0, 0, 0))

##--> no obvious colinearity problem !
```

Since there was no strong evidecnes of colinearity, we stopped here for now.

## II. Partitioning the training set
By partitioning the training set data (training, from **pml-training.csv**) to 2 subsets : training.train, and training.test, we would then be able to compare the out-of-sample accuracy among the choices of models, before deciding on the final model, which would then be assessed by the testing set (**pml-testing.csv**) 

```{r echo=TRUE, message=TRUE, fig.width=6, fig.height=6}
# Within the training set, further partition it into train and test sets
set.seed(12345)
inTrain <- createDataPartition(training$classe, p=0.7, list=FALSE)
training.train <- training[inTrain, ]
training.test  <- training[-inTrain, ]

dim(training.train)
dim(training.test)
```
## III. Modeling
### Method 1: Random Forests Approach
First, Random Forests modeling is attempted. Notice the use of parallel computing and a change from default resampling method to imrpove the computing speed.

```{r, message = TRUE, eval=FALSE}
set.seed(12345)

#Configure parallele computing 
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 4) #leave 4 cores for others
registerDoParallel(cluster)

##changing the resampling method from the default of bootstrapping to k-fold (k=5) cross-validation
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE, verboseIter=FALSE)
Modelfit1 <- train(classe ~ . ,method="rf", data=training.train, trControl= fitControl )
save(Modelfit1, file = "Modelfit1.RData")
##De-register parallel processing cluster
stopCluster(cluster)

```

```{r echo=FALSE, message=TRUE}
load("Modelfit1.RData")
```
```{r echo=TRUE, message=TRUE}
Modelfit1
confusionMatrix.train(Modelfit1)  ##--> accuracy at 0.9985
```
```{r echo=TRUE, message=FALSE, fig.show='hold', fig.height=4, fig.width=4}
par(mfrow=c(1,2))
plot(Modelfit1) ## --> sample accuracy peaks at 0.9985, with 40 predictors
plot(varImp(Modelfit1), top = 10)
```

Use this model to make prediction on training.test
```{r echo=TRUE, message=TRUE, fig.show='hold', fig.height=5, fig.width=5.5}
# prediction on training.test dataset
pred1test <- predict(Modelfit1, newdata=training.test)
confusionMatrix1 <- confusionMatrix(pred1test, training.test$classe)
confusionMatrix1  	##--> accuracy at 0.9975

###plot confusion matrix
plot(confusionMatrix1$table,  col = NULL, cex=0.6, main = paste("Confusion Matrix (Random Forests) \n Accuracy =", round(confusionMatrix1$overall['Accuracy'], 4)))
```

The out-of-sample accuracy was an impressive 0.9988 !

### Method 2 : Generalized Boosted Model
Again, paralel computing was used for efficient processing.
```{r, message = TRUE, eval=FALSE}
set.seed(12345)

#Configure parallele computing 
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 4) #leave 4 cores for others
registerDoParallel(cluster)

fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
Modelfit2 <- train(classe ~ ., data=training.train, method = "gbm", trControl = fitControl, verbose = FALSE)
save(Modelfit2, file = "Modelfit2.RData")
##De-register parallel processing cluster
stopCluster(cluster)

```
```{r, echo = FALSE, message = FALSE}
load("Modelfit2.RData")
```
```{r, echo = TRUE, message = TRUE}
confusionMatrix.train(Modelfit2)    ####--> accuracy at 0.9963
```
```{r echo=TRUE, message=TRUE, fig.height=4, fig.width=4}
plot(varImp(Modelfit2), top = 10)
```

Use this model to make prediction on training.test

```{r echo=TRUE, message=FALSE, fig.height=5, fig.width=5.5}
# prediction on training.test dataset
pred2test <- predict(Modelfit2, newdata=training.test)
confusionMatrix2 <- confusionMatrix(pred2test, training.test$classe)
confusionMatrix2  	##--> accuracy at 0.9949

###plot confusion matrix
plot(confusionMatrix2$table,  col = NULL, cex = 0.6, main = paste("Confusion Matrix (Generalized Boosted Model) \n Accuracy =", round(confusionMatrix2$overall['Accuracy'], 4)))
```

The out-of-sample accuracy was also an impressive 0.9964 !

Both of Random Forests and Generalized Boosted Model had close to perfect out-of-sample accuracy

# Results
Two methods were a virtual tie. But since Random Forests model had a slight nominal adavntage, I chose Random Forests to make the final prediction on the provided testing set (**pml-testing.csv**, N=20).

```{r echo=FALSE, message=FALSE}
##apply the selected model (Random forest) to the test data
predictTestData <- predict(Modelfit1, newdata=test)
predictTestData
```
This set of prediction achieved a 100% accuracy ! 
(Similarly and not surprisingly, the GBM model made the same predictions for the final tes set of 20)



